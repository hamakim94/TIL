# Deep Learning Recap

결국 classification

hyperplane(구분하는 선)

새로운 데이터들이 들어오면 판별이 가느함

하나는 Linear_function (Wx+B ) + Step_function(0,1 판별하기)

## Error Function

solution에서 얼만큼 멀어지느냐!!

에러를 최대한 낮추는 방법! ( Gradient Descent) -> 산 모양에서 내려가기



## 근데 Non-Linear라면?

심지어 연속형이면,  확률로 표현하고 싶어서 step function을 sigmoid로 나타내서 -0과 1 사이 값을 표현하기로 했따



## 근데 클래스가 여러개라면 어떻게 할까..

### softmax란?

score는, 항상 양의 값으로 주고 싶어서, 각 score들에 exp를 취하는데..

이걸 softmax라고 한다.



## 연속형 데이터

sigmoid로 표현하면 확률이 구해질 수 있다..!

따라서 각 데이터 별로 어디 있을 확률을 구할 수 있으므로

이걸 가능도(Likelihood) 라고 한다

그럼, 모든 데이터에 있어서 각각 가능도를 높이는게 좋겠지

그래서 다 곱해서 계산을 하고싶은데, 문제는 0과 1사이 값을 계속 곱하니

데이터가 너무 많아져서 아주 작으면 안좋으니 로그를 취해서 더해줘

근데 음수가 나오므로 , 낮춰줘서

따라서 cross entropy는 -log를 취해서, 낮으면 낮을수록 좋다!



## 클레스가 3개 이상이면..

마찬가지로 각각 클래스에 대해서 

Multi-Class Cross Entropy

로그를 취해서 볼록한 모형까지 만들어지기에, GD 사용 가능

근데 데이터가 만약 너무나 많아.

배치를 만들어 평균을 구하자.

#### issue

Momentum, 등이 있지만 Adam 쓰기 , Local Minimum



# 이제 Back Propagation

