{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "from gensim import models\n",
    "from gensim.models import Word2Vec\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize(sentence):\n",
    "    okt = Okt()\n",
    "    tokenized_sentence = []\n",
    "    # 우선 단어의 기본형으로 모두 살리고, 명사, 동사, 영어만 담는다.\n",
    "    # 그냥 nouns로 분리하는 것보다 좀 더 정확하고 많은 데이터를 얻을 수 있다.\n",
    "    for line in sentence:\n",
    "        result = []\n",
    "        temp_sentence = okt.pos(line, norm=True, stem=True) # 먼저 형태소 분리해서 리스트에 담고\n",
    "\n",
    "        for i in temp_sentence:                             \n",
    "            if (i[1] == 'Noun' or i[1] == 'Adjective' or i[1] == 'Alpha'):                  \n",
    "                result.append(i[0])\n",
    "            \n",
    "        tokenized_sentence.append(result)\n",
    "\n",
    "    return tokenized_sentence\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
