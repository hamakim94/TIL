# 머신러닝

**가설**과, 그 가설의 **비용(예측값과 실제값의 차이)의 최소화**

underfitting : train data만으로 예측하지 못 할 때

overfitting : train data 에 과도하게 맞아서, 새로운 데이터 예측이 별로(need 규제)
=> 해결 : Cost 함수의 규제를 추가



## 1_지도 학습

### 1_1. Linear Regression => 회귀

최소제곱법 사용

![스크린샷 2023-11-08 오전 9.07.12](https://p.ipic.vip/io22oi.png)

> https://ko.wikipedia.org/wiki/%EC%B5%9C%EC%86%8C%EC%A0%9C%EA%B3%B1%EB%B2%95

#### overfitting 문제

RIDGE(L2 norm) : 0에 수렴
입력변수가 전반적으로 비슷한 수준으로 출력변수에 영향을 미칠 때

LASSO(L1) : 0이 됨 -> feature selection(중요한 feature만 남는다)
출력변수의 영향을 미치는 입력변수의 영향력 편차가 큰 경우

cf) Elastic-Net : Lasso + Ridge 하이브리드 모델



### 1_2. Logistic Regression => 분류

**분류문제를 선형으로 해결**(선형의 Decision Boundary, 결정 경계를 선형으로)

선형함수에 0과 1 사이의 범위를 갖는 함수를 합성!

H : sigmoid => cost : 조건부 활용
복잡한 Cost Function에 log를 조건부로 합성 -> Gradient Descent 사용 가능

다중 분류
class 여러개 -> OvO(이진분류 여러개 중 확률 가장 높은 것 선택)
OvR -> Softmax함수 사용

분류에서 Perfomance Measure => 정밀도, 재현율, F1 score 등등



### 1_3. DecisionTree => 회귀, 분류

Information Gain(IG) : Tree 결정 기준
순도가 높도록(불순도가 낮도록) => 그 중 entropy만 학습

어떤 결정(선형 분류 기준)이 더 좋을까?

![스크린샷 2023-11-08 오전 9.17.10](https://p.ipic.vip/jd45qv.png)

Recursive Partitioning(반복적 분할) => Entropy를 낮추는 방식으로  Feature 계속 분할
문제점 : 과적합

Prunning을 통해 해결(사실, 세부 가지를 병합, 즉 완전 비슷한 것들을 하나로 합침)



#### Ensemble

#### **Bagging(Bootstrap Aggregating)**

단순하게 약한 학습기 독립적으로 합치기
BootStrap : 원본 데이터 집단으로부터 원본가 유사한 새로운 데이터 집단 추출
Aggregate : 합치다, 모으다
원본 데이터로부터 중복을 허용하여 무작위로 N개의 데이터 추출 -> 샘플데이터

서로 다른 약한 학습기(Decision Tree)의 예측값을 합치는 것

##### **Random Forest(Sets of Tree)**

부트스트랩으로 만들어진 여러 샘플 데이터를 가지고 여러번의 Decision Tree를 수행 -> 이를 합친다

1. 학습 데이터 N개로부터 중복을 허용하여 무작위로 샘플 데이터 N개 추출
2. 샘플데이터에서 중복을 허용하지 않고 무위로 X값(input feature) d개를 선택
3. d개의 feature에 대해 DecisionTree 수행
4. 위의 1~3 고정을 K번 수행

#### **Boosting**(More error, More weight)

연계적(순차적으로 업데이트)
Bagging과 처럼 K번의 독립적 샘플링, **오차가 발생한 데이터에 가중치를 더 강하게**, 다음 샘플링 수행
**오차가 발생했던 데이터가 다음 번에 또 뽑한다면 또 틀리지는 않도록 학습**



##### **AdaBoost(Adaptive Boosting)**

1. 학습 데이터 N개로부터 중복 허용하여 무작위로 샘플 데이터 N개 추출
2. 샘플 데이터에서 중복 허용 X, 무작위로 X값(input feature) d개를 선택한다
3. d개의 feature에 대해 Decision Tree 수행
4. 오차가 발생한 데이터가 샘플링할 때 더욱 잘 추출되도록 가중치 부여
5. 위의 1~3 과정을 K번 수행

##### Gradient Boosting

AdaBoost : 높은  Weight를 가지는 data point가 있으면 성능이 크게 떨어짐
Gradient Boosting은 AdaBoost처럼 반복마다 데이터 가중치를 수정하는 대신
**이전 예측 모델에서 계산된 Residual Cost(잔여 오차)를 경사하강법으로 새로운 예층 모델 학습**

오래걸리는 단점, Boosting 자체가 다 오래걸림

**XGBoost가 위 단점들 개선**



### 1_4. SVM => 분류

![스크린샷 2023-11-08 오전 10.48.31](../../../Library/Application Support/typora-user-images/스크린샷 2023-11-08 오전 10.48.31.png) > https://en.wikipedia.org/wiki/Support_vector_machine

결정 경계를 선형으로 그리되, 데이터와 선분의 폭을 크게 하자
**일정한 마진 오류 안에서, 두 클래스 간의 도로 폭(마진)이 가능한 최대**

SVM regression
도로 안(마진)에 가능한 많은 샘플이 들어가도록 학습

**Kernal SVM**

![SVM](https://p.ipic.vip/3s0589.png) > https://en.wikipedia.org/wiki/Support_vector_machine

선형으로 구분 못 해 -> 차원을 늘려(Feature Engineering)
기저 함수를 사용해 차원을 늘림(기저 함수 -> 내적해서 사용)
다양한 기저함수가 존재한다



## 2_비지도 학습

데이터의 Y값이 없다( = 라벨값이 없다 = 타겟 벨류가 없다 = 종속 변수가 없다)

### 2_1. Clustering

#### k-means : K개의 평균 군집 찾기

![스크린샷 2023-11-08 오전 10.46.38](https://p.ipic.vip/s40eep.png)

> https://en.wikipedia.org/wiki/K-means_clustering

**Expectation / Maximization**
E : 중심점 기준으로 편을 가르는 것
M : 편을 기준으로 중심점 재설정 => 반복 수행



#### Hierachical Clustering

병합 클러스터링

1. 데이터 하나를 하나의 클러스터로 간주
2. 각 데이터 간의 모든 유사도 계산
3. 유사도가 높은(거리가 가까운) 클러스터들들 통합해나가며 클러스터의 총 수를 줄여나감

특징

1. 데이터 간의 유사도를 먼저 계산
2. 계산복잡도 높음 O(n^3)

다양한 유사도 계산법이 있음(ward, complete, average, centroid)



### 2_2. PCA

![스크린샷 2023-11-08 오전 10.44.57](https://p.ipic.vip/0ks4sz.png)

> https://medium.com/@raghavan99o/principal-component-analysis-pca-explained-and-implemented-eeab7cb73b72

데이터의 입력값, 입력 특성(Feature)을 변환하는 비지도변환
차원 축소 목적, 주 알고리즘으로 PCA 있음

**차원의 저주란?**
벡터 공간 차원은 엄청 크지만, true data는 작은 차원의 공간으로 표현해도 충분한 경우

**데이터의 특성을 가장 잘 나타내주는 주성분 찾기**
**분산을 가장 크게 갖는 벡터**
