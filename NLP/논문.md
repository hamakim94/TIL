by ysubg 1gram - 5gram  -> we can consider the context

(Lower unemplyment, sluggish recovery)



in order to distunguish certain language, we're going to use 2 kinds of sentiment indicators

- marketing approach
  - pros : do not rely on the researcher's subjecteive selection of seed words
    -  use only market informations
  - cons: naturally produce statistically significant outcomes
- lexical approach
  - pros : polarity based on proximity(근접)  to the pre-determined seed words



So we use state-ofthe-art domain-specific sentiment induction algorithm

'SentProp'



'eKoNLPy'



previous : frequency //// rely more on tones/sentiments





!!방법론!!

## 1.2 MPB MINUTES

- 말해주는 내용
  - Outline은 행정적인것을 다루고
  - 현 경제상황 담화 요약은 MPB 구성원들의 경제적 상황에 대한 말, FX와 국제 금융, 금융 사정과 통화 정책을 담고 있다
  - 통화 정책 결정에 대한 담화는 각 개인의 시점을 담고 있다
  - May 2005 to December 2017

We collect *231,699 documents for the period of May 2005–December
2017, which include 151 minutes of MPB meetings, 206,223 news
articles, and 26,284 bond analyst reports. Table 2 shows the types and
numbers of documents and the average and maximum number of
sentences. While our target texts are the MPB minutes, we use a large
amount of other documents to build field-specific lexicons.



##  1.2News Articles

금리라는 키워드로 *네이버와 *연합 인포맥스에서 January 2005 to December 2017 기간동안 검색했다.

- 일반 경제, 통화 정책, 금융 시장과 BOK의 미래 통화 정책  입장(stance)에 대해 대중적인 인식이 담겨있음

총 206,223개이고,  42% (86,538) 는 연합 인포맥스 , 33% (68,728) 은 이데일리, and 25% (50,957) 는 연합뉴스에 담겨있다.



## 1.3 Bond Analysts’Reports 채권 분석가들의 보고서

사용한 이유는 2가지인데..

- 1. 통화 정책과 채권 시장에 대해 전문적인 견해가 나타나 있기 떄문
  2. 일반 전문가보다 편하게 글을 씀 -> lexicon(사전)에 넣기 위해서 !!

WIEfn, https://www.wisereport.co.kr/

corpus : we extract using
Latent Dirichlet Allocation (LDA) method, a topic modeling method. Table



# 2. Pre-processing Texts 전처리!

## 2.1 Typical Steps of Pre-Processing

1. tokenize(하나하나 짜르기) -> pos tag( 자른거에 품사를 붙여!)

2. normalize  구두점을 지우거나, 자주 사용하지 않는 단어를 빼고, 동의어들은 stemming(increasing - increas)과	 lemmatization(better - good으로!) 과 case folding을 통해서 하나로 묶어 센다!

   (생각을 해봅시다. 만약 좋아 긍정적 대박 초대박 을 다 다른 단어를 세면)

   통화정책 대박! 통화정책 초대박! 정책 아주 긍정적, 이런것들은 그냥 긍정이라는 키워드로 묶으면 후에 n-gram을 통해 비둘기스와 매스들로 나누기 편하겠죠?





## 2.2 Korean NLP Python Library for Economic Analysis (eKoNLPy)

1. use eKoNLPy(이유)

   1. 후치(Postposition)가 띄어쓰기로 구분돼있지 않은데, 심지어 띄어쓰기도 짜증나! -> 일반적인 언어(영어식)로 tokenize가 불가능하다고 생각해~
   2. 외래어가 있는데, 외국 표준과 맞지 않게 자기들 멋대로 써버려, 특히 전문적인 용어들은 더 지멋대로야
   3. 동의어들인데 지멋대로 바꿔써!  .. 아니 inflation을 인플레 물가 인플레이션으로 쓴다니까? 이거 n-gram 쓸 때 매우 짜증나!
      - 왜냐면 동의어들이 다 다른 n-gram으로 만들어 져서 n-gram의 빈도수를 미친듯이 상승시켜 ㅠ
   4.  동사와 형용사들이 불규칙적으로 합쳐져.. -> n-gram 모델 개수 미친 폭팔..

   첫번째 문제(띄어쓰기) 사실 Konlpy 쓰면 되는데, 나머지 문제들 때문에 우린 특정한 사전들이 필요해. 그래서 만든거야! 잘 써 , 겁나 잘만듦!

2. 한국어의 미친듯한 동사 변화를 처리하기 위해 stemming보다는 we use lemmatizition ( reason),  왜냐면 비슷한 형태를 하나로 묶는데, 심지어 eKoNLPY는 1291개나 되는 경제와 금융 도메인에서 많이 쓰이는 형용사와 동사를 갖췄다는 사 실 !



# 3. Feature Selection

 솔직히 글쓴이 : 김민균 이라는게 내 의견을 말해주진 않잖아, 그치? 필요 없지?

이런거 지우면 속도도 빨라지고 단어들을 벡터화한것들의 차원까지 줄여주는 1 석 2 조 

---------그런데말입니다..?

회복 자체는 분명 긍정적인데.. fucking 느린 회복은 .. .. 긍정적으로 느껴지진 않죠..? -> 처리 하는거에 어려움을 느껴..

' N - G R A M   두 둥 등 장 ' ( N개의 토큰화된 단어를 하나로 뭉쳐서 해석할래!)

근데 만약 10그램(10개의 단어를 한 뭉치로 보면) 너 무 길 어.. 너무 구체적인 문장이 하나의 단위로 세지기 때문에 다른 문서에서 쓰이기가 어렵고, 데이터도 너무나 많아져..

' 차 원 의 저 주 두 둥 등 장'

그래서 우리는 1~5그램만 쓸거야! 근데.. 5가지 제한 조건을 곁들인.



1. (NNG), adjectives (VA, VAX), adverbs (MAG), verbs (VA), and negations. 만 쓸거고
2. 15번보다 적게 나온 단어는 가소롭다고 생각해서 안 씀



The final word set is comprised of 2,712 words and we obtain the
resulting 73,428 n-grams.

using 1 gram to 5 gram



# 4. Polarity Classification(흑 백 논 리 )

우리 스스로 n-gram에서 흑백을 따져야해.. 

불황? 응 너 비둘기. 꾸륵꾸륵..(매우 약하다) (불황 -> 사람들이 돈을 안써 -> 금리를 낮춰 -> 경제를 돌아가게 해)

호황? 응 너 매 .(마치 거대한 매가 돈을 낚아채듯, 돈이 많으니 우리 은행에 돈좀 넣어줘서 시중에 돈좀 없애줄래? ( 공짜로?) 아니 폭팔적인 금리로! . 이자 5억. ㅇㅋ 돈 다 넣음 (인플레이션 방지)



우리가 뽑은 단어들이 , 매를 예측하는지, 비둘기를 예측하는지 알고싶은데.. 긍정 부정 사전이 필요하네.,.?

뷁! 없어! 우리 스스로 만들어야해!!



만들기 위해 고려해야 할것

## 1st issue : supervise vs unsupervised ( 인간의 간섭이 필요하니?)

- 예시!!

supervised : Google Cloud Sentiment Analysis API ( 기존 많은 문서들을 해석해서 자기멋대로 해준거 + - 나눈거)

unsupervised : PMI (Pointwise Mutual Information) ( 컴퓨터가 알아서 계산해줘~)

## 2nd issue : machine-learning-based vs. lexical-based(어휘 기반)

- 사실 어휘 기반은 1.메뉴얼대로, 2.사전을 통해, 그리고 3.Corpus( **corpus** (literally Latin for body) refers to a collection of texts.) 그니까 우리가 문장을 만들 때 쓰는 자주 쓰는 단어들 -> 즉 , 문장의 핵심 , 문장의 핵, 이거 있으면 그나라 어휘 못해도 문장 만듬. ting bu dong, 이쿠라데쓰까?(얼마입니까) 감사합니다 등

1. 메뉴얼 : 인간의 치명적인 오류가(장난) : 실수이 나기 쉽고  시간 너무 오래걸려
2. 사전의 핵심 단어를 찾고 그것의 동의어와 반의어를 찾아야해.. 이거사실 잘 만들어진 데이터베이스가 필요해
   - 근데 단점은 너무 전문적인 영역의 단어들은 잘 모태
3.  엄청 큰 언어의 body(corpus)에서 동시에 발생되는 단어들의 패턴을 뽑 ㅇ ㅏ
   - 그 도메인에 잘 맞게 쓸 수 있어. 



이러한 관점에서, 우리는 2가지 측정 단위를 쓰겠오.(N-gram 기반)

1. market approach that classifies polarity from market information using machine learning
2. corpus-based approach that classifies polarity using word(ngram) embedding seed words -> lexical



### 4.1MARKET APPROACH

1. 머신 러닝 기법과 실제 자료들(금리 자료)을 통해 예측할거얌!

아니 실제 주가랑 금융 시장이랑 연관이 있어서 하나를 통해 다른 하나를 예측하려고 하는데, 시중에 깔려있는 기사들이나 전문가들의 자료로는 예측을 못하겠어? 해버리자!!!



우와 제가디시Jegadeesh and Wu의 방법은 주관적인 판단으로 이루어지지 않아서 호 평을 받 습 니 다 .

-> 단어를 독립 변수를 보고 주가를 뭐였지 아무튼 종속변수로 봐서 무게를 측정하기로 했어ㅏ

단어들의 상대적인 무게감(빈도수를 통한)을 측정하기 위해.. 긍정적이고 뭔가 거대하면 무게를 크게 설정할거야

근데 학습시키기 위해 머신러닝 씀

For our market approach, we use the Na ̈ıve Bayes classifier (NBC), ( 수업시간에 배웠쥬? )

근데 우리 naive하게, 모든 단어들이 독립이라는 가정이라고 해서 쓸겁니다(그럼에도 성능 좋다고 들었쥬?)



현실.. 근데 지도학습이걸랑.. 그래서 그걸 하는데 사람들을 통해 뽑거나 전문가를 써서 라벨링을 해야하는데..

주관적으로 판단하고 노동도 아주 미친듯이 해서 돈도 많이 줘야하는데.. 나는.. 돈이 없는걸?

we label news articles and reports in our corpus as hawkish (dovish) if the
1-month change of Call rates is positive (negative) on the day they are
released.

 => 그래서 우리는 1달간 call rate의 변화량이 긍정이면 그냥 그걸 다 긍정의 sign으로 볼래!!

training set and a test set by 9:1 ratio.

4 millions 문장을 9:1비율로 나누고 ( train_test_split 기억나쥬?)

we repeat this procedure 30 times

이 과정을 30번 학습시켜 랜덤성을 주고

 and use the average of the polarity scores as a final one.

그리고 30번 나온 매점수들  평균내서(30개를 하나로) 하겠다~



polarity score(그냥 매비(매, 비둘기) 논리 라고 하겠음)

은 

N-GRAM에 있는 단어들 중에 feature : 해삼(해삼이 들어간 N-gram, 문장 block, window?)

매라고 분류된 단어들 중에 해삼이 있는거 / 비둘기라고 분류된 단어들 중에 해삼이 있는거

그니까, 흑백논리(매비 논리) 점수가 높다? 매라고 분류된 단어들 중에 해삼이 포함된 개수가 많아!

-> 넌 높은 매 점수를 줄거야.



We classify the polarity of our lexicon as hawkish (dovish) if the
polarity score is greater (less) than 1, excluding lexicon in the grey area
using intensity of 1.3 as a threshold.30) The final number of lexicon is
18,685 for hawkish and 21,280 for dovish. A sample of polarity lexicon is
provided in table 4.



### 4.2 Lexical Approach

main idea : if two words appear together frequently in the same context, they are likely to have the same polarity. Then the polarity of an unknown word can be determined

나는 해삼 멍게가 좋아. 특히 해삼과 멍개를 고추장에 찍어먹으면 진짜 맛있어

라는 문장가 있는데.. 두개의 단어가 같은 문맥에서 자주 나타난다면, 같은 극성을 띈다는 직 관 적 인 생 각

좋아 + 멍게 좋아 + 해삼 멍개 + / 멍개 고추장 + 마시따! 뭐 대강 이런 느낌으로 다 긍정적으로 본다

by calculating the relative frequency of co-occurrence with another word.



1. PMI (Pointwise Mutual Information).

2. SO-PMI (Semantic Orientation from PMI)



단점들이 있어..:

- 단순 같이 있는것만 기준으로 비둘기와 매를 따지기 때문에 반의어를 제대로 찾기 어렵다
  - -> we use ngram2vec by Zhao, Liu, Li, Li, and Du (2017) instead of word embedding.
  -  단어 자체를 넣어줘서 계산하는것 보다 ngram을 vectorize한게 더 효과가 좋다고 말하네!
- 특정 몇개의 seed-word(좋다) 로 결과가 완전히 뒤바뀐다
  - SentProp framework by Hamilton et al.
  - addresses this issue by bootstrapping seed words. 뭐 이런 방식으로 해결한대!



측정은 seed-word와 n-gram들과의 거리로 한다, 그런데 seed word는 두마리의 새. 매와 비둘기 뿐

따라서 이 두개의 상대적인 크기로 한다,

이것도 1 넘어가면 넌 매! 



ngram2vec 

 too many(344,293 unique n-grams with a minimum frequency limit of 25, which yield 410,902,512 pairs of n-grams (21.7 GB in size).)... 

유니~크한 N-gram, 심지어 25번 이상 나온것들을 고르는데도 오지게 많은(21.7 GB) 데이터가 나와ㅣ..

so we bootstrap by running our propagation 50times over 10 random equally sized subsets of  hawkish and dovish seed sets

그래서 bootstrap을 한다~ 

--- determination : polarity score is greater (less) than 1, 이걸 기준으로~

excluding lexicon in the grey area using intensity of 1.1 as a threshold. 이해 X



그래서 보니까, 2개의 접근 방식으로 유사성의 결과가 69%나 n-gram이 비슷하게 분류돼(매! 비둘기!)



## 4.3, Evaluation

the BOK Governor’s news conference about monetary policy decisions. 를 이용해 평가할건데

May 2009 to January 2018,

we manually label 2,341 sentences as hawkish, neutral, and dovish. 2341개 문장을 3개의 기준으로 나눴어

- we train a Na ̈ıve Bayes classifier with randomly selected
  60% of hawkish and dovish sentences
  
  - 나이브 베이지안 분류기로 매의 60%와 비둘기의 60%를 train_set으로 뽑아서 
  
- test with the remaining
  sentences. 나머지로 검증했어
  
  심 지 어 30 번 반 



With 30 times of iteration, the average accuracy of classifiers is
about 86%, which we think is above par accuracy. 매우 정확하게 해준다~~



# Measuring Sentiments (tones)

2 step approach

we calculate the tone of a sentence based on the
number of hawkish and dovish features (n-grams) in each sentence.



whole tone -> define tone of document

수식

creates a continuous variable  for each document, which is
bound between −1 (dovish) and +1 (hawkish).33)



1.  Can our lexicon-based indicators (tone(mkt) and tone(lex) explain the
   BOK’s current and future monetary policy decisions? In particular,
   do they have additional information that are not available in the
   existing macroeconomic data?
   - yes!
   - 

2. Is it important to use a field-specific dictionary?
3. Is it important to use the original Korean text, not Korean-to-English
   text?



