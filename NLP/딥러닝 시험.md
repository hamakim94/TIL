# 딥러닝 시험

#### perceptron(개요 ppt)

* perceptron -> 0.7,0.6(강의 초반) 나오기

* x의 다항식, W(weight)의 차원이 뭔지., b의 차원 묻는 문제.



* Error function의 성질 -> /GD 사용
* 미분을 하기 위해 실수로 나타내야함(연속) -> sigmoid를 사용



* 만약 여러개 분류 -> softmax(문제) 익스포낸셜 합에 특정 익스포넨샬

  ## [0.67,0.24,0] -> softmax([2,1,0])  -> 실습!!!!!!!!!!!!!!!!!!

* Maximum likelihood -> 각 확률의 곱 -> CrossEntropy 계산 한 문제
* 로그취해서 더해준다



* Gradient Descent 실습에서, 편미분 값이 의미하는것이 무엇인지
* wi - a*&dE/dW(딥러닝 기초 실습) -> Error Function, ( y - output)
* 잘 예측하면 output이 1이된다. -> 0에 가까워짐 -> Gradient 작아짐
* 잘못 예측하면 error 커져 ( 멀어지면 커져, 예측값과 가까우면 작아져)



#### 단어 임베딩(강의2)

원핫 인코딩, 이유

* 밀집 벡터 : "분포 가설을 기반으로"
* 유사하다 -> 코사인 유사도 값이 커진다



Word2Vec, 2가지 방법(윈도우 사이즈)

cbow(중심 단어 예측), skip-gram, 하나만 가지고 주변단어 다~

* abstract 관련 문제

* 단어의 연속적 벡터 표현의 2가지 모델 제안
* 단어 유사성으로 이 벡터



* Glove 문제 하나 더(왜 나왔는지, 어떤 문제점이 있었는지)

기존(LSA) : 전체적인 통계정보를 활용한다는 강점, 단어 유추 문제에 좋지 않은 성능이 단점

기존(Word2Vec) : 단어 유추 문제에선 비교적 좋은 성능을 보임, 단어사용 통계정보를 활용하지 않음

 => 임베딩된 단어 벡터간 유추문제에 좋은 성능을 보이며(Word2Vec)

 => 말뭉치 전체의 통계 정보를 반영하는것이 핵심 목표

조건부 확률로, 특정 단어가 사용된 문맥에서, 다른 단어도 사용됐을 확률을 상대적으로 계산

* co-occurence matrix : 단어 동시 등장 여부



* FASTTEXT(Word Embedding  기법)

기존 방법들은 언어의 형태학적 특성을 반영 못하고, 희소 단어들은 Embedding이 되지 않음

단어를 Bag-of-Characters로 보고, n-gramize된 Characters를 임베딩 -> 각 단어는 임베딩된 n-gram의 합(subword)의 개념

내부 단어까지 고려하는 학습

** 노이즈가 많은 데이터에 대해



결국 count_based지만, tf-idf,lsa보다 월등히 개선됌

------------------------static------------------------------------------------



#### 문맥적 단어 임베딩(3)

* 문맥적 단어 임베딩의 특징

문맥에 따라 vector를 만든다

같은 단어여도 문맥에 따라 다른 vector가 만들어 질 수 있음

같은 단어여도 다른 방식으로 표현돼



Tanh -> Vanashing -> Relu(히든 layer) 등장

[샘플의 수, 문장의 최대 길이, 단어의 차원]



#### 05 . CNN

다양한 필터를 추출함 (다양한 관점(필터))

* 필터 사이즈 몇으로 내면 아웃풋(형태), 슬라이딩-> 문제
* (N-F) / stride + 1

구현한 논문

-> CNN을 이용하여 NLP 문제 해결,  ngram처럼 뽑아내서 문맥 특징 살랴~





#### RNN

* 서로 비교하기(RNN LSTM GRU ) -> 문제



RNN -> 장기의존성 -> back-propa -> 0에 가까워짐(vanishing)( tanh)

LSTM : 정보를 얼마만큼 갖고 갈 것인지~(hidden_state, cell_state)

GRU : forget gate - input gate만, cell_state가 없지만 비슷함, 성능 좋음



#### seq2seq

- many to many
- 입력과 출력의 길이가 다르다*******************
- 컨텍스트 벡터(encoding layer에 마지막 hidden_state를 설명)
- 번역, 챗봇에 적합

- 단점 : 오래걸려

##### Attension

긴 문장에 대해 다 포함

어떤 단어를 번역하고자 할 때 내가 봐야하는 위치가 어딘지 결정하는 것

attension weight를 기반으로. 코사인 유사도도 쓰고, concat을 쓰기도 함

score hidden state -> softmax -> attension weight

단점 : 오래걸려





#### Transformer

RNN, CNN 없다, Attension만 가지고 해결(장기기억, 병렬처리) 다 가능!

인코더 층을 많이 쌓았다. 쿼리 가중치 행렬이 8개나 있어

8개의 다른 -> Scale-dot Productuin 

입력이 들어가면,  가중치 행렬을 곱해서 쿼리 행렬을 만들어

각 문장에 대해서 어텐션을 진행 -> dot_matrix -> scaling -> softmax 

-> 다 더해줌 ( 각각 하나씩이, 행렬곱으로 밀어넣어주면 바로 계산이 됍)

​               V

나는       ㅁ

내일       ㅁ

여행을   ㅁ

간다       ㅁ

##### Decoder

한 번에 각 단어씩 들어가, 마스킹을 씌워서, 어텐션 벡터를 구하지 않아도 대



디코더의 히든 스테이트가 -> masked multi head attension은 ViVk는 인코더에서 나와서 유사한 편



엘모 버트 X





