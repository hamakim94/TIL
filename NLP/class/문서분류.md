# 문서 분류

- 문서를 사전에 구성된 그룹으로 분류하는 모델
  - 카테고리 분류 감정 분석, 언어 탐지 등
- 텍스트 분류는 텍스트를 빠리고 비용 효율적으로 적용이 가능하다



| 경제 :                     | IT :                     | 라벨   |
| -------------------------- | ------------------------ | ------ |
| 한은, 추가 금리, 환율 상승 | 국가기술표준원, 인공지능 | 텍스트 |

- 학습데이터는 텍스트와 라벨로 구성
- 학습데이터를 훈련하고 새로운 문서가 입력되면 학습된 분류 방법에서 





## 분류 모델(1)  - 나이브 베이즈 분류

- 베이즈 정리를 사용하는 분류 모델
- 학습 데이터에서 추출한 이미 알고 있는 사전 확률을 바탕으로 



## 분류 모델(2) - SVM

## 분류 모델(3) - 딥러닝(CNN, RNN)





## 우리는 Bayes Classifier 

- 데이터의 조건부 확률에 기반한 분류 => 데이터 중심
- 범주형 자료에만 적용 가능 : 수치형 자료는 범주형 자료로 변환
- 좋은 성능을 위해서는 데이터가 필요하다



- 종류
  - Exact Bayes Classifier
    - 조건부 확률과 베이즈 확률에 기반
    - 조건이 많으면 계산이 어려움





### 확률이란?

- 어떤 사건이 발생할 가능성
- 확률 = 가능성 = %
- 어떤 사건이 발생할 가능성을 0~1 값으로 표현한 것
- P(A) = n(A) / n(S)
  - S : 표본 공간(Sample Space)
  - A : 사건(event)



- 조건부 확률?
- P(B|A) = 
  - A 조건이 주어진 상태에서 B가 발생 할 확률
  - A 사건이 발생한 이후, B 사건이 발생할 확률
    -  => 표본공간이 변경된다고 생각하면 돼!(A라는 사건이 일어났을 때!)



P(A & B) = P(A) * P(B|A)

P(A & B) = P(B) *P(A|B)

-> 따라서 교집합은 조건부 확률의 순차적 곱으로 분해가 가능해 -> 복잡한 사건을 분해해서 사용



### 나이브 베이즈 분류

특성들 사이의 독립을 가정하는 베이즈 정리를 적용



장점

- 범주형 변수 처리
- 단순성, 계산 효율성
- 좋은 분류성능



단점







### 베이즈 정리 

- 사전 확률과 사후확률 사이의 관계를 조건부 확률을 이용해 계산하는 확률 이론

사후 확률 : C가 나타난 이유를 알고싶어!

데이터를 알고 있을 때, 그걸 가능케 하는 확률이| 원인C  * 사전확률 P(C) / 

증거



- 사전 확률( 이미 알아)
  - 이미 알고 있는 사건이 발생할 확률
- 우도
  - 이미 알고 있는 사건이 발생한다는 조건 하에 다른 사건이 발생이 확률
- 사후 확률
  - 사전확률과 우도를 통해서 알게되는 조건부 확률





### Laplace-Smoothing

- 입력 테스트가 기존에 계산한 확률이 존재하지 않을 경우 0으로 계산될 수 있음
- 신규 단어가 입력되면 각 확률은 0으로 계산되니
  - 상수를 더해줘서 막아준다



### Log 이용 언더 플로우 방지

- 확률을 계산하고 확률간의 곱으로 연산이 이루어짐
- 1이하 값으로 이루어지는 값을 계속해서 곱하면 소수점 이하로 계속 작아서 계산할 수 없는 범위 이하로 작아지는 것을 말함
- Log의 성질을이용하여 곱샘을 덧셈으로 변환하여 underflow를 방지한다.



