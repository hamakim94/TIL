# 시험 

## 6. 단어의 표현

1. 원핫 인코딩
   - 단어를 숫자로 표현하고자 할 때 적용할 수 있는 간단한 방법론
   - 한계점이 있다(단어의 수만큼 차원 필요, 의미를 담지 못해(코사인 유사도 0))
2. BoW(Bag of Words)
   - TF-IDF(Term Frequency - Inverse Documnet Frequency)

### 단어 임베딩

정의 : 단어 임베딩은 단어의 의미를 간직하는 밀집 벡터(Dense Vector)로 표현하는 방법

- 밀집 벡터를 만드는 방법
  - '같은 문맥에서 등장한 단어는 유사한 의미를 가진다'

### 유사도 계산

1. 유클리디언 거리
2. 코사인 유사도( 1에 가까울수록 유사도가 높다 -> 유사한 의미를 가진다)
3. 자카드 유사도

### 유사도 계산

- N-gram
  - 복수개(n개) 단어를 보느냐에 따라 Unigram, Bigram, Trigram 등으로 구분
  - 제한적으로 문맥을 표현할 수 있다.
- 활용
  - 연어 처리 : 금융 통화 위원회, 국회 의원, 고객 서비스 등
  - Language Modeling에 사용
    - 분야(Domain)에 따라 단어들의 확률 분포는 다르다
    - 분야에 적합한 코퍼스를 사용하면 연어 모델의 성능이 높아진다



## 7.문서의 표현(Document/Sentence Representation)

문서의 표현이란?

- 문서를 자연어 처리를 위해 연산할 수 있도록 숫자로 표현하는 방법
- 문서를 벡터로 표현하는 방법

### BoW

문서 내 단어 출현 순서는 무시하고, '빈도수'만 기반으로 문서를 표현하는 방법

생성 방법

1. 각 토큰의 고유 인덱스를 부여한다(from collections import defaultdict)

2. 각 인덱스 위치에 토큰 등장 횟수를 기록한다

   (for sent in sentences

   ​	for token in sent

   ​		if token in dictionary:

   ​			dict[token] += 1)

한계점

- 단어의 순서를 고려하지 않는다
- BoW는 Spare함(벡터 공간의 낭비, 연산 비효율성을 초래)
- *단어 빈도수가 중요도를 바로 의미하지 않는다* . 단어가 자주 등장한다고 중요하진 X
- 전처리가 매우 중요. 같은 의미의 다른 단어 표현이 있으면.?



### TDM(Term-Document Matrix)

- Bag fo Words 중 하나
- 문서에 등장하는 각 단어 빈도를 행렬로 표현한 것

TDM이 있고 DTM이 있다(term : 단어, Document : 문장)

- 한계정
  - TDM 역시 Spare함,
  - 단어의 빈도수가 중요도를 바로 의미하지 않아
    - 그래서 우리는 TF-IDF 를 쓴다고





## TF-IDF

- 단어 빈도 - 역문서 빈도
- TDM 내 각 단어의 중요성을 가중치로 표현한다
- TDM을 사용하는 것보다 더 정확하게 문서비교기고 가능함
  - TF : 특정 문서 d에서의 특정 단어 t의 등장 횟수
  - IDF : 특정 단어 t가 등장한 문서의 수의 역수



TF가 높다 : 많이 등장 / IDF가 높다 : 다른 문서들에서 그 단어가 많이 안 나온다

=> 중요한 단어라구!



활용

- 정보 검색
  - 검색어와 가장 관련이 있는 문서를 찾아서 결과를 제공한다
- 키워드 추출
  - TF-IDF 점수가 가장 높은 점수를 가지고 있는 단어가 그 문서를 대표함

계산

- 가중치를 계산하는 방법이 여러개 있다.  표준화를 하거나, IDF는 로그를 취함
- log(N/nt)
- smooth : log(N / (1+nt))  + 1 



계산 절차

- 토큰 Index 생성 ( Defaultdict 활용)
- TF 계산:(각 문서별 토큰의 개수 합을 구하고, 문서내 토큰 빈도에 나눈다.)
- IDF 계산:(np.count_nonzero)를 활용해, 로그값을 계산한다
- TF-IDF 계산 : 두개 곱해





## 핵심 키워드 추출(Keyword-Extraction)

- 키워드 추출은 문서에서 가장 중요한 단어를 자동으로 추출하는 과정
- 중요한 단어를 추출한다 => 단어의 중요성을 어떻게 판단할 것인가



### 필요성

- 대량 데이터 처리 가능
  - 직접 문서를 읽고 주요 용어를 수동으로 식별하기엔 너무 많은 시간 소요
- 추출의 일관성
  - 키워드 추출은 규칙과 사용자가 정의한 매개변수를 기반 -> 일관적임
- 실시간 분석 가능
  - 특정 키워드 추출을 실시간으로 수행하고, 제품에 대한 의견을 반영 가능



### TF-IDF를 활용해서 핵심 키워드 추출

가능하다

### TextRank

- Google ( nodes / edge)

- 그래프 기반 Ranking 모델
- 키워드와 문장 추출을 위한 비지도 학습 방법을 제안
- 그래프 기반 랭킹 알고리즘은 그래프의 각 vertex(노드)의 중요도를 결정하는 방법
- 기본 아이디어는 하나의 vertex가 다른 vertex에 연결되면 투표 혹은 추천
- 가장 득표를 많이 한 vertex가 중요하다!
- undirected(score가 전부 1로 했을 때) 알아서 계산되는게 더 빠르게 수렴
- 이제 Keyword Extraction
  - 텍스트는 품사가 태깅돼 토큰화 됨
  - 단어 윈도에 동시 등장한 토큰 사이는 엣지를 추가하여 그래프를 생성
  - 0.0001를 threshold로 20~30회 반복
- *TextRank는 깊은 언어지식이나 도메인별 Corpora를 필요하지 않고, 다른 도메인, 장르, 언어에 적용할 수 있음*



## 문서 요약(Text Summarization)

- 문서 요약은 문서에서 중요한 문장을 자동으로 추출하는 과정
- 중요한 문장을 추출한다 -> 문장의 중요성을 어떻게 판단할 것인가!

### Luhn Summarize

- 단어의 중요성은 사용 빈도로, 작가는 중요한 단어를 반복해서 사용한다는 사실에 기반
- 문장 중요도
  - 중요 단어를 포함하는 경우에
  - 중요 단어가 등장하는 처음과 끝사이 단어들 중 중요 단어의 상대 비율
  - ex) 중요 단어 4, 윈도내 7개 -> 4^2/7
- 절차
  - 토큰화 
  - 중요단어 결정(0.001 < 단어빈도비율 < 0.5)
  - 문장 주요도 계산(문장내 포함된 중요단어 상대비율 계산 - window)
  - 문서 요약 : 문장 중요도 순위별 출력



### TextRank 활용 문서요약

- 문장을 추출하는것은 키워드 추출과 유사
- 두 방법 모드 텍스트를 대표하는 시퀸스를 식별하는 것을 목표로 함
- 문장내 co-occurence에 기반한 관계 정의는 적용 X(문장이기 때문에)
- 문장간 유사성이 있는 경우 connection이 있다고 정의하고, 유사성은 다른 함수로 측정
- 텍스트 내 다양한 문장 사이에 관계 강도가 결정, 이를 역순으로 정렬하여 요약한다

- 텍스트 단위의 로컬 컨택스트 + 전체 텍스트에서 재귀적으로 정보를 고려하기 떄문에 잘 작동

- 링크하는 다른 텍스트 단위의 중요성을 바탕으로 텍스트 단위를 평가

- 역시나 비지도학습의 장점(Corpora 필요 X, 다른데 활용 가능)

- 자카드 유사도를 통한 문장간 유사도를 측정

- 그래프 생성 -> 초기값을 기반으로 그래프를 만들어

- 여러번 iteration -> 초기 마지막을 기준

  

## 문서 분류(Documnet Classification)

- 문서를 사전에 구성된 그륩으로 분류하는 모델
  - 카테고리 분류 , 감정 분석, 언어 탐지 등
- 텍스트 분류는 텍스트를 빠르고 비용 효율적으로 적용이 가능하다
- 학습데이터는 텍스트와 라벨(분류)로 구성
- 학습 데이터를 훈련하고 새로운 문서가 입력되면 학습한 분류 내에서 문서의 분류를 예측



### 분류 모델1. 나이브 베이즈

- 베이즈 정리를 사용하는 분류 모델
- 학습 데이터에서 추출한 이미 알고 있는 사전 확률을 바탕으로 사후 확률을 계산하여 분류

### 분류 모델 2 - SVM

- SVM은 제한당 양의 데이터를 처리 할 때 좋은 성능을 보이는 분류 알고리즘
- 주어진 그룹에 속하는 벡터/ 아닌거와 분류를 결정
- 많은 학습 데이터가 필요하지 않지만, 나이브 베이즈 분류보다 좋은 성능을 위해선 더 많은 계산 리소스가 필요함

###  3 - DeepLearning

- 분류에는 널리 사용되는 주요 딥 러닝 모델인 CNN, RNN, Transformer 기반 모델
- 데이터가 많을수록 잘 작동하기 때문에 잘 Tagging된 데이터가 필요하다



### Bayes Classifier

- 데이터의 조건부 확률에 기반한 분류 -> 데이터 분류
- 범주형 자료에만 적용 가능 : 수치형 자료는 범주형으로 변환이 필요하다
- 좋은 성능을 위해서는 대량 데이터가 필요하다

Exact Bayes Classifier(계산 어려움) / Naive Bayes Classifier (독립변수 많을 떄 간단히 계산)



### 나이브 베이즈 분류

특성들 사이의 독립을 가정하는 베이즈 정리를 적용

대중적인 방법

- 사용자가 사전에 정의한 범주를 학습하여 새로운 문서가 입력됐을 때 범주 분류 예측



#### 장점

- 범주형 변수 처리
- 단순성, 계산 효울성
- 좋은 분류성능

#### 단점

- 많은 데이터 필요
- 값이 0일 확률 처리 : Laplace Smoothing



### Naive Bayes 개선 - Laplace smoothing

- 입력 텍스트가 기존에 계산한 확률이 존재하지 않을 경우 0으로 계산됨
- 분자와 분모에 일정 상수(k)를 더하여 신규 단어가 출현했을 때 0으로 계산되는거 방지
- (P(wi)|positive)= k + count(wi, positive) / 2k + sum(w,positive)

### 개선 2, Log이용 언더플로우 방지

- 확률을 계산하고, 확률간의 곱으로 연산이 이루어짐(Naive)
- 1 이하 값으로 이루어지는 값을 계속 곱하면 소수점 이하로 계속 작아서 계산할 수 없는 범위 이하로 작아지는 것을 언더플로우라고 함
- Log의 성질을 활용해 곱샘을 덧셈으로 변환해 다시 exp를 취해주면 ok



### 나이브 베이즈 성능 개선

- 불용어 처리 : 분류를 판단하는데 불필요한 단어를 제거해줘야 함
- 원형복원 : 같은 의미의 다른 표현을 원형복원하여 표준화
- N-gram : n개의 단어 묶음, 문맥 포함 가능
- TF-IDF : 단순 빈도 기반이 아니라 TFIDF 기반하여 단어 점수 결정도 가능하다.

