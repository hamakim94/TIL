{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec 문제점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['I', 'bow', 'my', 'head'], ['I', 'shoot', 'a', 'bow']] # 뜻이 다른 경우 어떻게 될까요\n",
    "\n",
    "model = Word2Vec(sentences,\n",
    "                size = 1,\n",
    "                window = 1,\n",
    "                min_count = 1) # 1번 이상 나온거~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 의 벡터 값 [-0.40560234]\n",
      "bow 의 벡터 값 [-0.40751427]\n",
      "my 의 벡터 값 [0.03190934]\n",
      "head 의 벡터 값 [-0.29370248]\n",
      "shoot 의 벡터 값 [0.45202494]\n",
      "a 의 벡터 값 [0.13696747]\n"
     ]
    }
   ],
   "source": [
    "word_vectors = model.wv\n",
    "vocabs = word_vectors.vocab.keys()\n",
    "\n",
    "for v in vocabs:\n",
    "    print(v, '의 벡터 값', word_vectors[v])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위를 보면 bow가 다른건데 하나만 나와...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문맥적 단어 임베딩 - 한계를 보완\n",
    "- 같은 단어여도 문맥에 따라 다른 벡터가 생성\n",
    "- Elmo, Bert, OpenAI, GPT등\n",
    "\n",
    "- 문장 자체를 입력받아 임베딩을 시키고싶다 이말이야.\n",
    "- 일반적으로 다계층이고. 문장을 구성하는 단어를 임베딩함\n",
    "- 심지어 좌, 우 방향도 고려해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 만약 데이터가 엄청 많으면 \n",
    "\n",
    "전체 데이터를 학습시키지 않고, 일부를 쓴다\n",
    "- 각 minibatch에 대해서 normailzation 해줘(정규화) , 그리고 local minimum 을 제외시키려고 노력한다(Momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "- Sigmoid : 0~1, layer 증가시 Gradient Vanishing 문제 발생 가능하다는거(여러개 곱해지니까\n",
    "- Tanh : -1~1 ,근데 마찬가지로 sigmoid와 같은 문제점들이 발생하기도 함 \n",
    "- Relu : >0 이면 기울기가 1, x<0이면 함수값이 0 \n",
    "-      : 학습이 빠름, 연산 비용 낮고, 구현도 간단, x<-인 값에 대해선 기울기가 0이기 때문에 뉴런이 학습에서 제외\n",
    "- Leaky Relu : \n",
    "\n",
    "## Tensor\n",
    "- 그냥 행렬이고, scalar, vector, matrix, 3-tensor, n-tensor 등이 있다\n",
    "### nlp\n",
    "- -tensor를 만이 쓴다,mini batch[ [ sentences [vector] ] ] (문장(리스트) 속 단어(리스트)를 하나인 리스트\n",
    "- (3, 2, 1) (문장의 수 ,단어의 수 , 단어 벡터의 차원) = (sample수, sentence 길이, 단어벡터\n",
    "    - max_sentences로 tensor의 형식을 지정해주는 편임, 그래서 패딩을 넣어줌\n",
    "    사실, ( 문장수, 가장 긴 문장의 길이, 단어벡터의 차원)\n",
    "\n",
    "### image\n",
    "- (3,5,5,1),(images, 5rows, 5columns, 1 channel(흑백)) (3,5,5,3):(images, 5rows, 5columns, 3 channel(RGB))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
