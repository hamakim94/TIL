{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가존 임베딩의 문제점\n",
    "- 단어유추 문제에서는 비교적 좋지만\n",
    "- 학습 데이터엣 관찰되는 단어 사용 통계정보를 활용하지 않는다\n",
    "- 지역적인건 가능하지만, Corpus에서 관찰되는 서로 다른 두 단어의 동시발생횟수에 기반한 학습은 할 수 없음\n",
    "\n",
    "## GLoVe\n",
    "- 임베딩된 단어 벡터간 유추문제에 좋은 성능 + 말뭉치 전체의 통계 정보(LSA) 둘 다 \n",
    "- 임베딩된 두 단어벡터의 내적이 말뭉치 전체에서의 동시 등장확률 로그값이 되도록 목적함수를 정의\n",
    "- 전체 까지 고려하니까 성능 더 좋아지더라\n",
    "\n",
    "## FastText\n",
    "- 페이스북에서 발표\n",
    "- 언어의 형태학적인 특성은 반영하지 못하고 희소 단어는 못해\n",
    "- 개별 단어가 아닌 n-gram의 특징(character)을 임베딩, 토큰 자체를 묶어 apple -> app, ppl, ple\n",
    "    - 이렇게 하면 oov(out of value) 뺄 수 있어\n",
    "- 성능이 빠르고 좋은 성능을 보임\n",
    "- 단어기준이아닌 약간 형태소를 기준으로 해서 해봅니다\n",
    "- 그래서 오타 섞인 단어도 약간 분석 가능하게 해준다\n",
    "\n",
    "    - 한국에 FastText\n",
    "- 한국어는 자모(초성,중성,종성)단위로 해도 성능 꽤나 좋다\n",
    "\n",
    "## 임베딩 비교\n",
    "NNLM - Word2Vec(skipgram, Cbow) - Glove - FastText - ? \n",
    "\n",
    "## 한계\n",
    "- 의미상 아무련 관련이 없는 단어도 벡터공간에 가깝게 인베딩(SW,HW)\n",
    "- 결국, 모두 count-based 방법\n",
    "- 하지만 TF-IDF, LSA보다 훨씬 좋아져서 각광\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
