{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make dict(by keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index # defauldict 그냥 한 번에 하네.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 make sentences to vect(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'You love my dog!',\n",
    "    'Do you think my dog is amazing?'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences # 그런데, 패딩 넣어줘야 하는게 맞잖아.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      "padded sequences = \n",
      "[[ 0  5  3  2  4]\n",
      " [ 0  5  3  2  7]\n",
      " [ 0  6  3  2  4]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, maxlen=5)\n",
    "print(\"\\nWord Index = \", word_index)\n",
    "print(\"\\nSequences = \", sequences)\n",
    "print(\"\\npadded sequences = \")\n",
    "print(padded)\n",
    "\n",
    "# default가 오른쪽부터임.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\n",
    "    'i really love my dog',\n",
    "    'my dog loves my manatee'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
      "\n",
      "Padded Test Sequence : \n",
      "[[0 0 0 0 0 5 1 3 2 4]\n",
      " [0 0 0 0 0 2 4 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"\\nTest Sequence = \", test_seq)\n",
    "\n",
    "padded = pad_sequences(test_seq, maxlen = 10)\n",
    "print(\"\\nPadded Test Sequence : \")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 3, Long (colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-04964c2761f7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-36-04964c2761f7>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    wget --no-check-certificate' https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json' -O /tmp/sarcasm.json\u001b[0m\n\u001b[1;37m                                                                                                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "wget --no-check-certificate' https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json' -O /tmp/sarcasm.json\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실습 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: tensorflow-metadata 0.23.0 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.9.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting absl-py\n",
      "  Using cached absl-py-0.9.0.tar.gz (104 kB)\n",
      "Requirement already satisfied: six in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.23.0-py3-none-any.whl (43 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (1.18.5)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading protobuf-3.13.0-cp36-cp36m-win_amd64.whl (1.1 MB)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.2.zip (177 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (2.24.0)\n",
      "Requirement already satisfied: future in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: attrs>=18.1.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (19.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow_datasets) (4.47.0)\n",
      "Collecting googleapis-common-protos\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from protobuf>=3.6.1->tensorflow_datasets) (47.3.1.post20200622)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Building wheels for collected packages: promise, absl-py, termcolor, dill\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21499 sha256=cd6b225e307636ffe5c9c68cca738206ba66f49f3ebca11c0ee530524eccded6\n",
      "  Stored in directory: c:\\users\\student\\appdata\\local\\pip\\cache\\wheels\\59\\9a\\1d\\3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121935 sha256=0a810eb76f708fcc1a0246311934bef3237adcd4c14882a3930784a6a3f2581b\n",
      "  Stored in directory: c:\\users\\student\\appdata\\local\\pip\\cache\\wheels\\c3\\af\\84\\3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=179851c276769eb4fb8b822afce0b17fe8b02eeadafce1e6138d7e646707b2d3\n",
      "  Stored in directory: c:\\users\\student\\appdata\\local\\pip\\cache\\wheels\\93\\2a\\eb\\e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.2-py3-none-any.whl size=78977 sha256=71e2fc19467275261470b7346430557523e9f35ca30e885756566af23994d016\n",
      "  Stored in directory: c:\\users\\student\\appdata\\local\\pip\\cache\\wheels\\02\\49\\cf\\660924cd9bc5fcddc3a0246fe39800c83028d3ccea244de352\n",
      "Successfully built promise absl-py termcolor dill\n",
      "Installing collected packages: promise, absl-py, protobuf, googleapis-common-protos, tensorflow-metadata, termcolor, dill, tensorflow-datasets\n",
      "Successfully installed absl-py-0.9.0 dill-0.3.2 googleapis-common-protos-1.52.0 promise-2.3 protobuf-3.13.0 tensorflow-datasets-3.2.1 tensorflow-metadata-0.23.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.0-cp36-cp36m-win_amd64.whl (342.5 MB)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.31.0-cp36-cp36m-win_amd64.whl (2.6 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (0.9.0)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Collecting scipy==1.4.1\n",
      "  Downloading scipy-1.4.1-cp36-cp36m-win_amd64.whl (30.8 MB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.20.1-py2.py3-none-any.whl (91 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (47.3.1.post20200622)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\student\\anaconda3\\envs\\nlp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Installing collected packages: rsa, cachetools, google-auth, grpcio, markdown, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, opt-einsum, astunparse, gast, tensorflow-estimator, google-pasta, keras-preprocessing, scipy, tensorflow\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.5.0\n",
      "    Uninstalling scipy-1.5.0:\n",
      "      Successfully uninstalled scipy-1.5.0\n",
      "Successfully installed astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.20.1 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.31.0 keras-preprocessing-1.1.2 markdown-3.2.2 opt-einsum-3.3.0 rsa-4.6 scipy-1.4.1 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\student\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71bc19bce1f49e28d0ce472419016c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0921ec00f8904e758efe92a792390e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\student\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteCL2ULG\\imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e233e9062a184297a80273756eb41d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\student\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteCL2ULG\\imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f18e6f93eb400fb3cf5f592844eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to C:\\Users\\student\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incompleteCL2ULG\\imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82dbc6e726a444591fb788d0e4bfa47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\student\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
